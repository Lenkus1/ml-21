{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use which loss?\n",
    "\n",
    "You can find an overview of implemented losses in the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Binary cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have a binary classification problem, you will typically have labels `True` and `False`, which equals to labels `1` and `0`. The values output by the last layer are called **logits** and take values in the interval $(-\\infty,+\\infty)$. For a classification task, we usually want our output to be a *probability*. To do this we need to rescale the logits to the interval $(0,1)$. For binary classification this is achieved by passing the logits through the **sigmoid** function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\sigma(x) = \\frac{1}{1+e^{-x}},\n",
    "\\end{equation}\n",
    "\n",
    "where is the logit from the output layer. the formula for the binary cross entropy is made up of two parts:\n",
    "\n",
    "- For the positive class, the base formula for cross entropy is $-y \\cdot log(\\hat{y})$ where $y$ is the ground truth (i.e. `1`), and $\\hat{y}$ is the predicted probability that your label is `1`.\n",
    "\n",
    "- For the negative class (i.e. `0`), we have $-(1-y)\\cdot log(1-\\hat{y})$.\n",
    "\n",
    "By putting these two pieces together we obtain the formula for binary corss entryopy:\n",
    "\n",
    "\\begin{equation}\n",
    "H(\\theta) = \\frac{1}{N}\\sum_{i=1}^N-(y \\cdot log(\\hat{y}) + (1-y)\\cdot log(1-\\hat{y})),\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the total number of examples in your dataset, and $\\theta$ are the parameters of your ML model. The dependence of the loss function on the parameters is implicit (i.e. the parameters do not appear explicitly in the formula of the loss function). Can you see which term in equation $(2)$ is implicitly carrying this dependence?\n",
    "\n",
    "If your true label is `1`, only the first part of the formula above will contribute to the tota loss. Can you see why?\n",
    "\n",
    "Viceversa, if your true label is `0`, only the second part of the formula above will contribute to the total loss.\n",
    "\n",
    "Let's see this in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051565782628\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "y = 1 # ground truth\n",
    "y_hat = 0.9 # prediction\n",
    "\n",
    "# first part of binary cross entropy\n",
    "print(-y * np.log(y_hat))\n",
    "# second part of binary cross entropy\n",
    "print(-(1-y) * np.log(1-y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens to the binary cross entropy for a single case when the probability to belong to the positive class `y_hat` changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = 1 # ground truth\n",
    "y_hat = np.linspace(0.1,0.9,9) # prediction\n",
    "\n",
    "# individual contribution of one prediction to the total loss\n",
    "bce = -( y * np.log(y_hat) + (1-y) * np.log(1-y_hat) )\n",
    "\n",
    "# visualization\n",
    "sns.barplot(x=y_hat, y=bce);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the further your prediction is to the true value of `1`, the greater its contribution to the total loss will be. This makes sense since we want a loss function that penalizes mistakes.\n",
    "\n",
    "How would you expect the above graph to look like for an example whose true value is `y=0`? Try to modify the code in the cell above and see whether the output is in line with your expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, this loss is implemented by the `BinaryCrossentropy` function. This function takes the option `from_logits` which specifies whether you are applying the function to the logits or the output probabilities. In case `from_logits=True`, the function will first pass the logits through the sigmoid and then compute the loss as in equation $(2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 11:12:28.353375: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.23101759>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0, 1, 0, 1]\n",
    "yhat = [0.1, 0.7, 0.3, 0.9] # probabilities\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we dont use a sigmoid, we can get the output of a linear model, which is called a **logit** and takes values in the range $(-\\infty, \\infty)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.00999736>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0, 1, 0, 1]\n",
    "yhat = [-10.3, 3.2, -17.18, 12.92] # logits\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "bce(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are two or more label classes in a one-hot encoding, you can use categorical cross entropy.\n",
    "\n",
    "Let's say we have three possible classes, and the label is the first class, we will have $[1, 0 ,0]$. If we predict the first class with high probability, but the second with small probability we could get something like $[0.95, 0.04, 0.01]$\n",
    "\n",
    "Note how the sum of the three proabbilities adds up to 1. This is a fundamental property of probabilities and, while in the binary case we enforced it by applying the sigmoid function to the logits, in the multiclass case we use the **softmax** function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^Ke^{x_j}},\n",
    "\\end{equation}\n",
    "\n",
    "where $K$ is the number of classes. Once we have the probability for each class, we can compute the categorical cross-entropy loss as:\n",
    "\n",
    "\\begin{equation}\n",
    "H(\\theta) = \\frac{1}{N}\\sum_{i=0}^N\\sum_{j=1}^K y_{i,j} \\cdot log(\\hat{y}_{i,j})\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of examples in the dataset, $K$ is the number of classes.\n",
    "\n",
    "If you have logits on the range $(-\\infty, \\infty)$ as output (i.e., you dont use an activation function) you can set `from_logits=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1769392"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[0, 1, 0], [0, 0, 1]] # one-hot encoded labels\n",
    "yhat = [[0.04, 0.95, 0.01], [0.1, 0.8, 0.1]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss(y, yhat).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse categorical cross-entropy\n",
    "\n",
    "In case your classes are not one-hot encoded, you can use `SparseCategoricalCrossentropy` to compute the categorical cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1769392"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1, 2] # labels not one-hot encoded\n",
    "yhat = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "scce(y, yhat).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class, Multi-label\n",
    "The most general case occurs when there are multiple classes to predict, and each instance can take multiple labels from the set of categories at the same time. E.g. you can have a three classes for movies ['commedy', 'sci-fi', 'horror'] and you are watching a movie that is both a comedy, and sci-fi. Your label will be $[1, 0, 1]$ and your prediction might be something like $[0.7, 0.5, 0.6]$. Or in the case of an x-ray of a chest: you might have pneumonia and/or cancer, or none of them.\n",
    "\n",
    "**Note**: for this case, the sum of the probabilities for each class does not necessarily have to add up to one because each class is not mutually exclusive. In this case, you should use the binary crossentropy. If you use a softmax, your values will sum to one. But that is not what you want! Because it is multilabel, you want to allow for multiple values to get close to one, so use a sigmoid as activation function. You can also use this with logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00449203, 0.00225358], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[1, 0, 1], [0, 0, 1]]\n",
    "y_pred = [[5.0, -10.0, 5], [-5.0, -10, 20]]\n",
    "loss = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0340551, 1.3391274], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[0, 1, 0], [0, 0, 1]]\n",
    "yhat = [[0.04, 0.95, 0.01], [0.1, 0.8, 0.1]]\n",
    "loss = tf.keras.losses.binary_crossentropy(y, yhat, from_logits=False)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "TODO: Write a brief intro about the characteristics of the loss function for regression problems.\n",
    "\n",
    "## Mean Absolute Error (MAE)\n",
    "\n",
    "The MAE is computed as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\hat{y}, y)=\\frac{1}{N}\\sum_{i=1}^N |y-\\hat{y}|.\n",
    "\\end{equation}\n",
    "\n",
    "The MAE measure the error in the same scale of the data, i.e. if you are predicting the price in dollars the unit of the MAE will be dollars as well. The distance of each prediction `y_hat` from the ground truth `y` contributes linearly to the total MAE. This implies that outliers are not heavily penalized. In case you want to penalize outliers heavily, you may want to consider other loss functions.\n",
    "\n",
    "- Why do we need to take the absolute value of `y-y_hat` to have a valid measure of the error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.326666666666666"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([10.2, 5.1, 8.12])\n",
    "yhat = np.array([2.2, 6.0, 9.2])\n",
    "loss = tf.keras.losses.MAE(y, yhat)\n",
    "\n",
    "assert np.array_equal(\n",
    "    loss.numpy(), np.mean(np.abs(y - yhat), axis=-1))\n",
    "\n",
    "loss.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, there is a clear outlier, which one? Let's introduce a new loss function that can be used it to penalize it better than what the MAE does.\n",
    "\n",
    "\n",
    "Try to change the outlier in the code, and note how the two loss functions react differently to the outlier.\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y}_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "Contrary to MAE, the units of the MSE are the square of the unit of the labels. For example, if you are predicting the area of a house in $m^2$, then your MSE will be in $m^4$. What is an obvious way to have the same units as those of the lables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.992133333333328"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.keras.losses.MSE(y, yhat)\n",
    "\n",
    "assert np.array_equal(\n",
    "    loss.numpy(), np.mean(np.square(y - yhat), axis=-1))\n",
    "\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above MSE > MAE because of the presence of an outlier. Do you understand why squaring the difference $y-y_hat$ penalizes outliers more than taking the absolute value as in the MAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Logarithmic Error\n",
    "\n",
    "If the target value has a huge spread, you might want to be easier on errors for the very large values. With this, you can use the mean squared logarithmic error:\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y)=\\frac{1}{m}\\sum_{i=1}^m ((log(y+1) -log(\\hat{y} + 1))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5336267729497567"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.keras.losses.mean_squared_logarithmic_error(y, yhat)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "These are just some of the many loss functions that can be used for classification and regression tasks. Often, the choice of the right loss function can improve the performance of the ML model because the way errors are compute affects the value of the gradients (remember backprop?) and these, in turn, affect how the model parameters are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food for thought\n",
    "\n",
    "- Can we use a regression loss, i.e. MAE for a classification problem?\n",
    "- What is the difference between a *loss* and a *metric*?\n",
    "- Can we use a metric as a loss? Can we use a loss as a metric?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55da07f3d1a7aa3ceac520ecfe12af6f97560c6fbaf7360a1866fd52ad3a227b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('opentag-f5tbS3t8-py3.8': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

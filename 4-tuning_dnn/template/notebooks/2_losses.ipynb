{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use which loss?\n",
    "\n",
    "You can find an overview of implemented losses in the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have a binary classification problem, you will typically have labels `True` and `False`, which equals to labels `1` and `0`. When using a sigmoid in your output layer, you will get values in the range $[0,1]$ which are interpreted as probailities.\n",
    "\n",
    "The base formula for cross entropy is $-y \\cdot log(p(y))$ where $y$ is the true label, and $p(y)$ is the predicted probability that your label is 1. The probability will be in the range $[0,1]$.\n",
    "\n",
    "If your label is 1, and you predict a probability of 0.9, you can calculate the binary cross entropy by \n",
    "multiplying the negative label with the log of the probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10536051565782628"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 1\n",
    "py = 0.9\n",
    "\n",
    "-y * np.log(py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a label 1, and predicted a low probability of 0.1, we need the loss to be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 1\n",
    "py = 0.1\n",
    "\n",
    "-y * np.log(py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, if the label is 0 and we predicted a low probability of 0.1, that would have been right. We can obtain this by subtracting from 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10536051565782628"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "py = 0.1\n",
    "\n",
    "-(1-y) * np.log(1-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "py = 0.9\n",
    "\n",
    "-(1-y) * np.log(1-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can combine these two situations in one formula. If the label is 1 or 0, one of the two parts will go to zero and will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = - \\frac{1}{N} \\sum_{i=1}^N y_i log(p(y_i)) + (1-y_i) log (1-p(y_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow implements this as the Binary Croosentropy loss. With `from_logits` set to false, the predicted value is expected to be on the range $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 14:46:58.803735: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.23101759>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0, 1, 0, 1]\n",
    "yhat = [0.1, 0.7, 0.3, 0.9]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we dont use a sigmoid, we can get the output of a linear model, which is called a **logit** and takes values in the range $[-\\infty, \\infty]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.00999736>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0, 1, 0, 1]\n",
    "yhat = [-10.3, 3.2, -17.18, 12.92]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "bce(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are two or more label classes in a one-hot encoding, you can use categorical cross entropy.\n",
    "\n",
    "Let's say we have three possible classes, and the label is the first class, we will have $[1, 0 ,0]$. If we predict the first class with high probability, but the second with small probability we could get something like $[0.95, 0.04,, 0.01]$\n",
    "\n",
    "Note how the sum of the three proabbilities adds up to 1. This is a fundamental property of probabilities and, while in the binary case we enforced it by applying the sigmoid function to the logits, in the multiclass case we use the **softmax** function.\n",
    "\n",
    "If you have logits on the range $[-\\infty, \\infty]$ as output (cf, you dont use an activation function) you can set `from_logits=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1769392"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[0, 1, 0], [0, 0, 1]]\n",
    "yhat = [[0.04, 0.95, 0.01], [0.1, 0.8, 0.1]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss(y, yhat).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse categorical cross entropy\n",
    "\n",
    "In the case of a lot of classes, a one-hot encoding can be impractical. So we can use a sparse representation, where we can write $[0,1,0]$ as 1, and $[0,0,1]$ as 2. If you have logits on the range $[-\\infty, \\infty]$ as output instead of probabilities you can set `from_logits=True`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1769392"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1, 2]\n",
    "yhat = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "scce(y, yhat).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class, Multi-label\n",
    "The most general case occurs when there are multiple classes to predict, and each instance can take multiple labels from the set of categories at the same time. E.g. you can have a three classes for movies ['commedy', 'sci-fi', 'horror'] and you are watching a movie that is both a comedy, and sci-fi. Your label will be $[1, 0, 1]$ and your prediction might be something like $[0.7, 0.9, 0.1]$. Or in the case of an x-ray of a chest: you might have pneumonia and/or cancer, or none of them.\n",
    "\n",
    "**Note**: for this case, the sum of the probabilities for each class does not necessarily have to add up to one because each class is not mutually exclusive.\n",
    "\n",
    "In this case, you should use the binary crossentropy. If you use a softmax, your values will sum to zero. But that is not what you want! Because it is multilabel, you want to allow for multiple values to get close to one, so use a sigmoid as activation function. You can also use this with logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00449203, 0.00225358], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[1, 0, 1], [0, 0, 1]]\n",
    "y_pred = [[5.0, -10.0, 5], [-5.0, -10, 20]]\n",
    "loss = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0340551, 1.3391274], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[0, 1, 0], [0, 0, 1]]\n",
    "yhat = [[0.04, 0.95, 0.01], [0.1, 0.8, 0.1]]\n",
    "loss = tf.keras.losses.binary_crossentropy(y, yhat, from_logits=False)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.992133333333332"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([10.2, 5.1, 8.12])\n",
    "yhat = np.array([5.2, 6.0, 9.2])\n",
    "loss = tf.keras.losses.MSE(y, yhat)\n",
    "\n",
    "assert np.array_equal(\n",
    "    loss.numpy(), np.mean(np.square(y - yhat), axis=-1))\n",
    "\n",
    "loss.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the square punishes outliers (can you find the outlier in the yhat?), the mean average error puts a smaller penalty on outliers.\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y)=\\frac{1}{m}\\sum_{i=1}^m |y-\\hat{y}|$$\n",
    "\n",
    "Try to change the outlier in the code, and note how the two loss functions react differently to the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3266666666666667"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([10.2, 5.1, 8.12])\n",
    "yhat = np.array([5.2, 6.0, 9.2])\n",
    "loss = tf.keras.losses.MAE(y, yhat)\n",
    "\n",
    "assert np.array_equal(\n",
    "    loss.numpy(), np.mean(np.abs(y - yhat), axis=-1))\n",
    "\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the target value has a huge spread, you might want to be easier on errors for the very large values. With this, you can use the mean squared logarithmic error:\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y)=\\frac{1}{m}\\sum_{i=1}^m ((log(y+1) -log(\\hat{y} + 1))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02543662"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1, 10, 1000]\n",
    "yhat = [1.2, 13, 1100]\n",
    "loss = tf.keras.losses.mean_squared_logarithmic_error(y, yhat)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that to a regular mse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3336.3467"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.keras.losses.MSE(y, yhat)\n",
    "loss.numpy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55da07f3d1a7aa3ceac520ecfe12af6f97560c6fbaf7360a1866fd52ad3a227b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('opentag-f5tbS3t8-py3.8': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
